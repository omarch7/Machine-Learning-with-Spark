{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314"
     ]
    }
   ],
   "source": [
    "val path = \"20news-bydate-train/*\"\n",
    "val rdd = sc.wholeTextFiles(path)\n",
    "val text = rdd.map{ case (file, text) => text }\n",
    "print(text.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rec.sport.hockey,600)\n",
      "(soc.religion.christian,599)\n",
      "(rec.motorcycles,598)\n",
      "(rec.sport.baseball,597)\n",
      "(sci.crypt,595)\n",
      "(rec.autos,594)\n",
      "(sci.med,594)\n",
      "(comp.windows.x,593)\n",
      "(sci.space,593)\n",
      "(sci.electronics,591)\n",
      "(comp.os.ms-windows.misc,591)\n",
      "(comp.sys.ibm.pc.hardware,590)\n",
      "(misc.forsale,585)\n",
      "(comp.graphics,584)\n",
      "(comp.sys.mac.hardware,578)\n",
      "(talk.politics.mideast,564)\n",
      "(talk.politics.guns,546)\n",
      "(alt.atheism,480)\n",
      "(talk.politics.misc,465)\n",
      "(talk.religion.misc,377)\n"
     ]
    }
   ],
   "source": [
    "val newsgroups = rdd.map{ case (file, text) =>\n",
    "    file.split(\"/\").takeRight(2).head\n",
    "}\n",
    "val countByGroup = newsgroups.map(n => (n, 1)).reduceByKey(_+_).collect.sortBy(-_._2).mkString(\"\\n\")\n",
    "println(countByGroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402978\n"
     ]
    }
   ],
   "source": [
    "val text = rdd.map { case (file, text) => text }\n",
    "val whiteSpaceSplit = text.flatMap(t => t.split(\" \").map(_.toLowerCase))\n",
    "println(whiteSpaceSplit.distinct.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addresses,,--,atheism,,cambridge.,<19930301143317@mantis.co.uk>\n",
      "lines:,290\n",
      "\n",
      "archive-name:,december,1.0\n",
      "\n",
      ",,,,,,,,,,,of,,,,,,,foundation\n",
      "\n",
      "darwin,assorted,atheist,are\n",
      "available,to:,(608),the,\"darwin,fish\".,it's,it's,symbol,,symbol,,like,like,stick,on,cars,,but,but,and,the,is,in,the,the,the,#4,,hollywood,\n",
      ",,san,darwin,gold,for,net,who,lynn,directly,,the\n",
      "price,the\n",
      "price,fish.\n",
      "\n",
      "american,books,critiques,critiques,the,bible,,of\n",
      "biblical,contradictions,,contradictions,,,and,american,0-910309-26-4,,edition,,atrocities,,contains,foote:,bible\n",
      "contradicts,aap.,king,american,,(512),,books\n",
      "\n",
      "sell,haught's,horrors\",(see,,york,york,newer,is:\n",
      "prometheus,is:\n",
      "prometheus,14228-2197.\n",
      "\n",
      "african-americans\n"
     ]
    }
   ],
   "source": [
    "println(whiteSpaceSplit.sample(true, 0.3, 42).take(100).mkString(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130126\n"
     ]
    }
   ],
   "source": [
    "val nonWordSplit = text.flatMap(t => t.split(\"\"\"\\W+\"\"\").map(_.toLowerCase))\n",
    "println(nonWordSplit.distinct.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wuair,schwabam,42b,125215,6j,he3,1pqd9hinnbmi,neurologists,jxicaijp,dwi,749,steaminess,dangers,qsins,instantaneous,391k,typeset,typeset,bippy,hollombe,mswin,diccon,4h0kj76,borg,g85,spe,kocharian,6097,1tbs,xs9,3zur,unaskable,9mf,cj1v,bowdoin,bowdoin,inre,inre,deadweight,deadweight,deterministic,createwindow,rockefeller,kjznkh,kjznkh,classifieds,ray_bourque,anachronistic,cherylm,005117,005117,005117,interfere,makewindow,mtearle,barking,ww2,vcrs,widmann,monger,projector,jdecarlo,warms,triangulate,triangulate,recieves,g45,rint69,rint69,herod,1496,libpackagexcl,6w8rg,6w8rg,00ecgillespie,phoniest,funded,canonical,ehs,birds,dxb132,xtappcontext,0iy4bn,lamers,023843,inconsitancies,isdres,trn,xa_rgb_default_map,dm9,rchzd2_8d,mtagm,walters,r1865,9gtf,9gtf,lfu1i9b,tyrell,tyrell,rvik\n"
     ]
    }
   ],
   "source": [
    "println(nonWordSplit.distinct.sample(true, 0.3, 42).take(100).mkString(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84912\n"
     ]
    }
   ],
   "source": [
    "val regex = \"\"\"[^0-9]*\"\"\".r\n",
    "val filterNumbers = nonWordSplit.filter(token => regex.pattern.matcher(token).matches)\n",
    "println(filterNumbers.distinct.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntuvax,dpsi,singen,leymarie,_congressional,fowl,rlhzrlhz,afterward,ignore,hcq,beleive,goofed,arax,dfuller,nondiscriminatory,steaminess,urtfi,urtfi,za_,tiems,bellevue,typeset,armegedon,gunning,croissant,yearsley,dolphin,tic,worshippers,theoreticians,siumv,arresed,borg,sunprops,sask,sask,subcircuits,subcircuits,uninjured,uninjured,internship,pws,keysym,vfj,vfj,connecters,spe,octopi,bhjn,winsor,winsor,winsor,yan,astonished,miserable,eng,subtleties,createwindow,silvers,explorers,antisemites,classifieds,ray_bourque,inviting,inviting,apply,cfsmo,holdren,holdren,mishandles,feszcm,rootx,scramblers,scramblers,nkm,hfd,makewindow,formac,exhausting,responsbible,paradijs,fuenfzig,hindenburg,trial,tact,fahrenheit,projector,jdecarlo,ndallen,recoend,ffbv,bracing,wy,herod,sonunda,sonunda,depicted,iauc,iauc,floor\n"
     ]
    }
   ],
   "source": [
    "println(filterNumbers.distinct.sample(true, 0.3, 42).take(100).mkString(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(the,146532)\n",
      "(to,75064)\n",
      "(of,69034)\n",
      "(a,64195)\n",
      "(ax,62406)\n",
      "(and,57957)\n",
      "(i,53036)\n",
      "(in,49402)\n",
      "(is,43480)\n",
      "(that,39264)\n",
      "(it,33638)\n",
      "(for,28600)\n",
      "(you,26682)\n",
      "(from,22670)\n",
      "(s,22337)\n",
      "(edu,21321)\n",
      "(on,20493)\n",
      "(this,20121)\n",
      "(be,19285)\n",
      "(t,18728)\n"
     ]
    }
   ],
   "source": [
    "val tokenCounts = filterNumbers.map(t => (t, 1)).reduceByKey(_+_)\n",
    "val orderingDesc = Ordering.by[(String, Int), Int](_._2)\n",
    "println(tokenCounts.top(20)(orderingDesc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ax,62406)\n",
      "(i,53036)\n",
      "(you,26682)\n",
      "(s,22337)\n",
      "(edu,21321)\n",
      "(t,18728)\n",
      "(m,12756)\n",
      "(subject,12264)\n",
      "(com,12133)\n",
      "(lines,11835)\n",
      "(can,11355)\n",
      "(organization,11233)\n",
      "(re,10534)\n",
      "(what,9861)\n",
      "(there,9689)\n",
      "(x,9332)\n",
      "(all,9310)\n",
      "(will,9279)\n",
      "(we,9227)\n",
      "(one,9008)\n"
     ]
    }
   ],
   "source": [
    "val stopwords = Set(\"the\", \"a\", \"an\", \"of\", \"or\", \"in\", \"for\", \"by\", \"on\", \"but\", \"is\", \"not\", \"with\", \"as\", \"was\", \"if\", \"they\", \"are\", \"this\", \"and\", \"it\", \"have\", \"from\", \"at\", \"my\", \"be\", \"that\", \"to\")\n",
    "val tokenCountsFilteredStopwords = tokenCounts.filter{ case (k, v) => !stopwords.contains(k) }\n",
    "println(tokenCountsFilteredStopwords.top(20)(orderingDesc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ax,62406)\n",
      "(you,26682)\n",
      "(edu,21321)\n",
      "(or,14686)\n",
      "(subject,12264)\n",
      "(com,12133)\n",
      "(lines,11835)\n",
      "(can,11355)\n",
      "(organization,11233)\n",
      "(re,10534)\n",
      "(what,9861)\n",
      "(there,9689)\n",
      "(all,9310)\n",
      "(will,9279)\n",
      "(we,9227)\n",
      "(one,9008)\n",
      "(would,8905)\n",
      "(do,8674)\n",
      "(he,8441)\n",
      "(about,8336)\n"
     ]
    }
   ],
   "source": [
    "val tokenCountsFilteredSize = tokenCountsFilteredStopwords.filter{ case(k, v) => k.size >= 2}\n",
    "println(tokenCountsFilteredSize.top(20)(orderingDesc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(hcq,1)\n",
      "(rohde,1)\n",
      "(_slightly_,1)\n",
      "(wuair,1)\n",
      "(mowtu,1)\n",
      "(bruns,1)\n",
      "(luminous,1)\n",
      "(beckmans,1)\n",
      "(arax,1)\n",
      "(fowl,1)\n",
      "(jxicaijp,1)\n",
      "(rlhzrlhz,1)\n",
      "(aces,1)\n",
      "(steaminess,1)\n",
      "(wargame,1)\n",
      "(qsins,1)\n",
      "(schwabam,1)\n",
      "(urtfi,1)\n",
      "(_congressional,1)\n",
      "(costner,1)\n"
     ]
    }
   ],
   "source": [
    "val orderingAsc = Ordering.by[(String, Int), Int](-_._2)\n",
    "println(tokenCountsFilteredSize.top(20)(orderingAsc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sina,2)\n",
      "(akachhy,2)\n",
      "(mvd,2)\n",
      "(sarkis,2)\n",
      "(wendel_clark,2)\n",
      "(relieves,2)\n",
      "(purposeful,2)\n",
      "(hizbolah,2)\n",
      "(wout,2)\n",
      "(uneven,2)\n",
      "(senna,2)\n",
      "(subdivided,2)\n",
      "(bushy,2)\n",
      "(feagans,2)\n",
      "(coretest,2)\n",
      "(oww,2)\n",
      "(historicity,2)\n",
      "(mmg,2)\n",
      "(margitan,2)\n",
      "(defiance,2)\n"
     ]
    }
   ],
   "source": [
    "val rareTokens = tokenCounts.filter{ case (k,v) => v < 2 }.map{ case (k, v) => k }.collect.toSet\n",
    "val tokenCountsFilteredAll = tokenCountsFilteredSize.filter { case (k, v) => !rareTokens.contains(k) }\n",
    "println(tokenCountsFilteredAll.top(20)(orderingAsc).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51802\n"
     ]
    }
   ],
   "source": [
    "println(tokenCountsFilteredAll.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51801\n"
     ]
    }
   ],
   "source": [
    "def tokenize(line: String): Seq[String] = {\n",
    "    line.split(\"\"\"\\W+\"\"\")\n",
    "    .map(_.toLowerCase)\n",
    "    .filter(token => regex.pattern.matcher(token).matches)\n",
    "    .filterNot(token => stopwords.contains(token))\n",
    "    .filterNot(token => rareTokens.contains(token))\n",
    "    .filter(token => token.size >= 2)\n",
    "    .toSeq\n",
    "}\n",
    "\n",
    "println(text.flatMap(doc => tokenize(doc)).distinct.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(mathew, mathew, mantis, co, uk, subject, alt, atheism, faq, atheist, resources, summary, books, addresses, music, anything, related, atheism, keywords, faq)\n"
     ]
    }
   ],
   "source": [
    "val tokens = text.map(doc => tokenize(doc))\n",
    "println(tokens.first.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[60] at map at HashingTF.scala:76"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{ SparseVector => SV }\n",
    "import org.apache.spark.mllib.feature.HashingTF\n",
    "import org.apache.spark.mllib.feature.IDF\n",
    "\n",
    "val dim = math.pow(2, 18).toInt\n",
    "val hashingTF = new HashingTF(dim)\n",
    "val tf = hashingTF.transform(tokens)\n",
    "tf.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262144\n",
      "706\n",
      "WrappedArray(1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0)\n",
      "WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115, 3166)\n"
     ]
    }
   ],
   "source": [
    "val v = tf.first.asInstanceOf[SV]\n",
    "println(v.size)\n",
    "println(v.values.size)\n",
    "println(v.values.take(10).toSeq)\n",
    "println(v.indices.take(10).toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706\n",
      "WrappedArray(2.3869085659322193, 4.670445463955571, 6.561295835827856, 4.597686109673142, 8.932700215224111, 5.750365619611528, 2.1871123786150006, 5.520408782213984, 3.4312512246662714, 1.7430324343790569)\n",
      "WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115, 3166)\n"
     ]
    }
   ],
   "source": [
    "val idf = new IDF().fit(tf)\n",
    "val tfidf = idf.transform(tf)\n",
    "val v2 = tfidf.first.asInstanceOf[SV]\n",
    "println(v2.values.size)\n",
    "println(v2.values.take(10).toSeq)\n",
    "println(v2.indices.take(10).toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,66155.39470409753)\n"
     ]
    }
   ],
   "source": [
    "val minMaxVals = tfidf.map { v =>\n",
    "    val sv = v.asInstanceOf[SV]\n",
    "    (sv.values.min, sv.values.max)\n",
    "}\n",
    "val globalMinMax = minMaxVals.reduce { case ((min1, max1), (min2, max2)) =>\n",
    "    (math.min(min1, min2), math.max(max1, max2))\n",
    "}\n",
    "println(globalMinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(0.9965359935704624, 1.3348773448236835, 0.5457486182039175)\n"
     ]
    }
   ],
   "source": [
    "val common = sc.parallelize(Seq(Seq(\"you\", \"do\", \"we\")))\n",
    "val tfCommon = hashingTF.transform(common)\n",
    "val tfidfCommon = idf.transform(tfCommon)\n",
    "val commonVector = tfidfCommon.first.asInstanceOf[SV]\n",
    "println(commonVector.values.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(5.3265513728351666, 5.308532867332488, 5.483736956357579)\n"
     ]
    }
   ],
   "source": [
    "val uncommon = sc.parallelize(Seq(Seq(\"telescope\", \"legislation\", \"investment\")))\n",
    "val tfUncommon = hashingTF.transform(uncommon)\n",
    "val tfidfUncommon = idf.transform(tfUncommon)\n",
    "val uncommonVector = tfidfUncommon.first.asInstanceOf[SV]\n",
    "println(uncommonVector.values.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val hockeyText = rdd.filter{ case (file, text) => file.contains(\"hockey\")}\n",
    "val hockeyTF = hockeyText.mapValues(doc => hashingTF.transform(tokenize(doc)))\n",
    "val hockeyTfIdf = idf.transform(hockeyTF.map(_._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08332038223731995\n"
     ]
    }
   ],
   "source": [
    "//Compute cosine similarity of two document vectors that belong to the same category\n",
    "import breeze.linalg._\n",
    "val hockey1 = hockeyTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\n",
    "val breeze1 = new SparseVector(hockey1.indices, hockey1.values, hockey1.size)\n",
    "val hockey2 = hockeyTfIdf.sample(true, 0.1, 43).first.asInstanceOf[SV]\n",
    "val breeze2 = new SparseVector(hockey2.indices, hockey2.values, hockey2.size)\n",
    "val cosineSim = breeze1.dot(breeze2) / (norm(breeze1) * norm(breeze2))\n",
    "println(cosineSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011982956191037503\n"
     ]
    }
   ],
   "source": [
    "//Compute cosine similarity of two documents that belong to different categories\n",
    "val graphicsText = rdd.filter { case (file, text) => file.contains(\"comp.graphics\")}\n",
    "val graphicsTF = graphicsText.mapValues(doc => hashingTF.transform(tokenize(doc)))\n",
    "val graphicsTfIdf = idf.transform(graphicsTF.map(_._2))\n",
    "val graphics = graphicsTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\n",
    "val breezeGraphics = new SparseVector(graphics.indices, graphics.values, graphics.size)\n",
    "val cosineSim2 = breeze1.dot(breezeGraphics) / (norm(breeze1) * norm(breezeGraphics))\n",
    "println(cosineSim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013522460083035466\n"
     ]
    }
   ],
   "source": [
    "val baseballText = rdd.filter { case (file, text) => file.contains(\"baseball\")}\n",
    "val baseballTF = baseballText.mapValues(doc => hashingTF.transform(tokenize(doc)))\n",
    "val baseballTfIdf = idf.transform(baseballTF.map(_._2))\n",
    "val baseball = baseballTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\n",
    "val breezeBaseball = new SparseVector(baseball.indices, baseball.values, baseball.size)\n",
    "val cosineSim3 = breeze1.dot(breezeBaseball) / (norm(breeze1) * norm(breezeBaseball))\n",
    "println(cosineSim3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[107] at map at <console>:88"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.classification.NaiveBayes\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "val newsGroupsMap = newsgroups.distinct.collect().zipWithIndex.toMap\n",
    "val zipped = newsgroups.zip(tfidf)\n",
    "val train = zipped.map{ case (topic, vector) => LabeledPoint(newsGroupsMap(topic), vector)}\n",
    "train.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Train the model\n",
    "val model = NaiveBayes.train(train, lambda = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Loading Test DataSet\n",
    "val testPath = \"20news-bydate-test/*\"\n",
    "val testRDD = sc.wholeTextFiles(testPath)\n",
    "val testLabels = testRDD.map { case (file, text) =>\n",
    "    val topic = file.split(\"/\").takeRight(2).head\n",
    "    newsGroupsMap(topic)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Transforming Test DataSet\n",
    "val testTf = testRDD.map { case (file, text) =>\n",
    "    hashingTF.transform(tokenize(text))\n",
    "}\n",
    "val testTfIdf = idf.transform(testTf)\n",
    "val zippedTest = testLabels.zip(testTfIdf)\n",
    "val test = zippedTest.map { case (topic, vector) => LabeledPoint(topic, vector)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7915560276155071\n",
      "0.7810675969031116\n"
     ]
    }
   ],
   "source": [
    "//Evaluating Model\n",
    "val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))\n",
    "val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()\n",
    "val metrics = new MulticlassMetrics(predictionAndLabel)\n",
    "println(accuracy)\n",
    "println(metrics.weightedFMeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7648698884758365\n",
      "0.7653320418573546\n"
     ]
    }
   ],
   "source": [
    "//Raw Features\n",
    "val rawTokens = rdd.map { case (file, text) => text.split(\" \")}\n",
    "val rawTF = rawTokens.map(doc => hashingTF.transform(doc))\n",
    "val rawTrain = newsgroups.zip(rawTF).map{ case (topic, vector) => LabeledPoint(newsGroupsMap(topic), vector)}\n",
    "val rawModel = NaiveBayes.train(rawTrain, lambda = 0.1)\n",
    "val rawTestTF = testRDD.map { case (file, text) => hashingTF.transform(text.split(\" \"))}\n",
    "val rawZippedTest = testLabels.zip(rawTestTF)\n",
    "val rawTest = rawZippedTest.map { case (topic, vector) => LabeledPoint(topic, vector)}\n",
    "val rawPredictionAndLabel = rawTest.map(p => (rawModel.predict(p.features), p.label))\n",
    "val rawAccuracy = 1.0 * rawPredictionAndLabel.filter(x => x._1 == x._2).count() / rawTest.count()\n",
    "println(rawAccuracy)\n",
    "val rawMetrics = new MulticlassMetrics(rawPredictionAndLabel)\n",
    "println(rawMetrics.weightedFMeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Word2Vec\n",
    "import org.apache.spark.mllib.feature.Word2Vec\n",
    "val word2vec = new Word2Vec()\n",
    "word2vec.setSeed(42)\n",
    "val word2vecModel = word2vec.fit(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(glens,1.3159770807119657)\n",
      "(ecac,1.3063226990531056)\n",
      "(woofers,1.2244201648489768)\n",
      "(sport,1.192043594806107)\n",
      "(ahl,1.1814434726624985)\n",
      "(roster,1.1619491231343353)\n",
      "(hispanic,1.15425795039727)\n",
      "(commissioner,1.1538891320841023)\n",
      "(golf,1.1537870644739696)\n",
      "(homeruns,1.1333313901547062)\n",
      "(assistant,1.129485586835786)\n",
      "(tournament,1.1223773487423643)\n",
      "(playoff,1.1143282937310677)\n",
      "(ncaa,1.1106336082384496)\n",
      "(champs,1.101362200963179)\n",
      "(rec,1.0981824063192052)\n",
      "(captains,1.0975511191200764)\n",
      "(surprises,1.094262378108031)\n",
      "(motorcycles,1.0774580362567963)\n",
      "(boxscores,1.0735494981958151)\n"
     ]
    }
   ],
   "source": [
    "word2vecModel.findSynonyms(\"hockey\", 20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(accommodates,0.9109466519258614)\n",
      "(briefed,0.8575139231706556)\n",
      "(amended,0.8537385855799084)\n",
      "(agency,0.8405774651559488)\n",
      "(advocate,0.8399973310548887)\n",
      "(papers,0.8260221616946232)\n",
      "(policies,0.8233629198783513)\n",
      "(aclu,0.8207162878765116)\n",
      "(procurement,0.8200093598026995)\n",
      "(journals,0.8186656379374622)\n",
      "(rkba,0.8181851162363095)\n",
      "(officials,0.8090259489409931)\n",
      "(cooperation,0.8046469700111981)\n",
      "(director,0.8035926081300374)\n",
      "(senate,0.8034863889546195)\n",
      "(amendments,0.7998425571269364)\n",
      "(layman,0.7994458023255351)\n",
      "(privacy,0.7990481700027887)\n",
      "(nren,0.7989127478009489)\n",
      "(enact,0.7976532149877671)\n"
     ]
    }
   ],
   "source": [
    "word2vecModel.findSynonyms(\"legislation\", 20).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
