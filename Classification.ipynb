{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rawData = sc.textFile(\"stumbleupon/train_noheader.tsv\")\n",
    "val records = rawData.map(line => line.split(\"\\t\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7395\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val data = records.map { r => \n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val features = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "data.cache\n",
    "val numData = data.count\n",
    "println(numData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Data for Naive Bayes, with all the negative values changed to 0.0\n",
    "val nbData = records.map { r => \n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val features = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble).map(d => if (d < 0) 0.0 else d)\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n",
    "import org.apache.spark.mllib.classification.SVMWithSGD\n",
    "import org.apache.spark.mllib.classification.NaiveBayes\n",
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "import org.apache.spark.mllib.tree.configuration.Algo\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "\n",
    "val numIterations = 10\n",
    "val maxTreeDepth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Logistic Regression model Training\n",
    "val lrModel = LogisticRegressionWithSGD.train(data, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Support Vector Machine model Training\n",
    "val svmModel = SVMWithSGD.train(data, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Naive Bayes model Training\n",
    "val nbModel = NaiveBayes.train(nbData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Decision Tree model Training\n",
    "val dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1.0\n",
      "True Label: 0.0\n"
     ]
    }
   ],
   "source": [
    "//Generating predictions\n",
    "val dataPoint = data.first\n",
    "val prediction = lrModel.predict(dataPoint.features)\n",
    "println(\"Prediction: \" + prediction)\n",
    "val trueLabel = dataPoint.label\n",
    "println(\"True Label: \" + trueLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0,1.0,1.0,1.0,1.0\n"
     ]
    }
   ],
   "source": [
    "//Predictions in bulk\n",
    "val predictions = lrModel.predict(data.map(lp => lp.features))\n",
    "println(predictions.take(5).mkString(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.5146720757268425\n"
     ]
    }
   ],
   "source": [
    "//Evaluation\n",
    "val lrTotalCorrect = data.map{ point => \n",
    "    if(lrModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "val lrAccuracy = lrTotalCorrect / numData\n",
    "println(\"Logistic Regression Accuracy: \" + lrAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.5146720757268425\n",
      "Naive Bayes Accuracy: 0.5803921568627451\n",
      "Decision Tree Accuracy: 0.6482758620689655\n"
     ]
    }
   ],
   "source": [
    "val svmTotalCorrect = data.map{ point => \n",
    "    if(svmModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "val nbTotalCorrect = nbData.map{ point =>\n",
    "    if(nbModel.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val dtTotalCorrect = data.map{ point =>\n",
    "    val score = dtModel.predict(point.features)\n",
    "    val predicted = if(score > 0.5) 1 else 0\n",
    "    if(predicted == point.label) 1 else 0\n",
    "}.sum\n",
    "\n",
    "val svmAccuracy = svmTotalCorrect / numData\n",
    "println(\"SVM Accuracy: \" + svmAccuracy)\n",
    "val nbAccuracy = nbTotalCorrect / numData\n",
    "println(\"Naive Bayes Accuracy: \" + nbAccuracy)\n",
    "val dtAccuracy = dtTotalCorrect / numData\n",
    "println(\"Decision Tree Accuracy: \" + dtAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
      "SVMModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\n",
      "NaiveBayesModel, Area under PR: 68.0851%, Area under ROC: 58.3559%\n",
      "DecisionTreeModel, Area under PR: 74.3081%, Area under ROC: 64.8837%\n"
     ]
    }
   ],
   "source": [
    "//Metrics\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "val metrics = Seq(lrModel, svmModel).map{ model => \n",
    "    val scoreAndLabels = data.map{ point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "val nbMetrics = Seq(nbModel).map{ model =>\n",
    "    val scoreAndLabels = nbData.map{ point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if(score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "val dtMetrics = Seq(dtModel).map{ model =>\n",
    "    val scoreAndLabels = data.map{ point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if(score>0.5)1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "val allMetrics = metrics ++ nbMetrics ++ dtMetrics\n",
    "allMetrics.foreach{ case (m, pr, roc) =>\n",
    "    println(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4122580529952672,2.761823191986608,0.4682304732861389,0.21407992638350232,0.09206236071899916,0.04926216043908053,2.255103452212041,-0.10375042752143335,0.0,0.05642274498417851,0.02123056118999324,0.23377817665490194,0.2757090373659236,0.615551048005409,0.6603110209601082,30.07707910750513,0.03975659229208925,5716.598242055447,178.75456389452353,4.960649087221096,0.17286405047031742,0.10122079189276552]\n"
     ]
    }
   ],
   "source": [
    "//Feature standardization\n",
    "import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
    "val vectors = data.map(lp => lp.features)\n",
    "val matrix = new RowMatrix(vectors)\n",
    "val matrixSummary = matrix.computeColumnSummaryStatistics()\n",
    "println(matrixSummary.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.045564223,-1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.999426,363.0,1.0,1.0,0.980392157,0.980392157,21.0,0.25,0.0,0.444444444,1.0,0.716883117,113.3333333,1.0,1.0,100.0,1.0,207952.0,4997.0,22.0,1.0,1.0]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10974244167559001,74.30082476809639,0.04126316989120241,0.02153343633200108,0.009211817450882448,0.005274933469767946,32.53918714591821,0.09396988697611545,0.0,0.0017177410346628928,0.020782634824610638,0.0027548394224293036,3.683788919674426,0.2366799607085986,0.22433071201674218,415.8785589543846,0.03818116876739597,7.877330081138463E7,32208.116247426184,10.45300904576431,0.03359363403832393,0.006277532884214705]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5053.0,7354.0,7172.0,6821.0,6160.0,5128.0,7350.0,1257.0,0.0,7362.0,157.0,7395.0,7355.0,4552.0,4883.0,7347.0,294.0,7378.0,7395.0,6782.0,6868.0,7235.0]\n"
     ]
    }
   ],
   "source": [
    "println(matrixSummary.numNonzeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n",
      "[1.137647336497678,-0.08193557169294771,1.0251398128933331,-0.05586356442541689,-0.4688932531289357,-0.3543053263079386,-0.3175352172363148,0.3384507982396541,0.0,0.828822173315322,-0.14726894334628504,0.22963982357813484,-0.14162596909880876,0.7902380499177364,0.7171947294529865,-0.29799681649642257,-0.2034625779299476,-0.03296720969690391,-0.04878112975579913,0.9400699751165439,-0.10869848852526258,-0.2788207823137022]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.mllib.feature.StandardScaler\n",
    "val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)\n",
    "val scaledData = data.map(lp => LabeledPoint(lp.label, scaler.transform(lp.features)))\n",
    "println(data.first.features)\n",
    "println(scaledData.first.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel\n",
      "Accuracy:62.0419%\n",
      "Area under PR: 72.7254%\n",
      "Area under ROC: 61.9663%\n"
     ]
    }
   ],
   "source": [
    "//Retrain Logistic Regression model with rescaled data\n",
    "val lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations)\n",
    "var lrTotalCorrectScaled = scaledData.map{ point => \n",
    "    if(lrModelScaled.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val lrAccuracyScaled = lrTotalCorrectScaled / numData\n",
    "val lrPredictionVsTrue = scaledData.map{ point =>\n",
    "    (lrModelScaled.predict(point.features), point.label)\n",
    "}\n",
    "val lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionVsTrue)\n",
    "val lrPr = lrMetricsScaled.areaUnderPR\n",
    "val lrRoc = lrMetricsScaled.areaUnderROC\n",
    "println(f\"${lrModelScaled.getClass.getSimpleName}\\nAccuracy:${lrAccuracyScaled * 100}%2.4f%%\\nArea under PR: ${lrPr * 100.0}%2.4f%%\\nArea under ROC: ${lrRoc * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map(\"weather\" -> 0, \"sports\" -> 6, \"unknown\" -> 4, \"computer_internet\" -> 12, \"?\" -> 11, \"culture_politics\" -> 3, \"religion\" -> 8, \"recreation\" -> 2, \"arts_entertainment\" -> 9, \"health\" -> 5, \"law_crime\" -> 10, \"gaming\" -> 13, \"business\" -> 1, \"science_technology\" -> 7)\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "//Adding additional features\n",
    "val categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap\n",
    "val numCategories = categories.size\n",
    "println(categories)\n",
    "println(numCategories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])\n"
     ]
    }
   ],
   "source": [
    "val dataCategories = records.map { r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val categoryIdx = categories(r(3))\n",
    "    val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "    categoryFeatures(categoryIdx) = 1.0\n",
    "    val otherFeatures = trimmed.slice(4, r.size - 1).map( d => if(d==\"?\") 0.0 else d.toDouble )\n",
    "    val features = categoryFeatures ++ otherFeatures\n",
    "    LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "println(dataCategories.first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575]\n",
      "[-0.02326210589837061,2.7207366564548514,-0.4464212047941535,-0.22052688457880879,-0.028494000387023734,-0.2709990696925828,-0.23272797709480803,-0.2016540523193296,-0.09914991930875496,-0.38181322324318134,-0.06487757239262681,-0.6807527904251456,-0.20418221057887365,-0.10189469097220732,1.137647336497678,-0.08193557169294771,1.0251398128933331,-0.05586356442541689,-0.4688932531289357,-0.3543053263079386,-0.3175352172363148,0.3384507982396541,0.0,0.828822173315322,-0.14726894334628504,0.22963982357813484,-0.14162596909880876,0.7902380499177364,0.7171947294529865,-0.29799681649642257,-0.2034625779299476,-0.03296720969690391,-0.04878112975579913,0.9400699751165439,-0.10869848852526258,-0.2788207823137022]\n"
     ]
    }
   ],
   "source": [
    "val scalerCats = new StandardScaler(withMean = true, withStd = true).fit(dataCategories.map(lp => lp.features))\n",
    "val scaledDataCats = dataCategories.map(lp =>\n",
    "    LabeledPoint(lp.label, scalerCats.transform(lp.features))\n",
    ")\n",
    "println(dataCategories.first.features)\n",
    "println(scaledDataCats.first.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel\n",
      "Accuracy: 66.5720%\n",
      "Area under PR: 75.7964%\n",
      "Area under ROC: 66.5483%\n"
     ]
    }
   ],
   "source": [
    "//Retrain Logistic Regression model with expanded feature set\n",
    "var lrModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats, numIterations)\n",
    "val lrTotalCorrectScaledCats = scaledDataCats.map{ point =>\n",
    "    if(lrModelScaledCats.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData\n",
    "val lrPredictionsVsTrueCats = scaledDataCats.map{ point =>\n",
    "    (lrModelScaledCats.predict(point.features), point.label)\n",
    "}\n",
    "val lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats)\n",
    "val lrPrCats = lrMetricsScaledCats.areaUnderPR\n",
    "val lrRocCats = lrMetricsScaledCats.areaUnderROC\n",
    "println(f\"${lrModelScaledCats.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaledCats * 100}%2.4f%%\\nArea under PR: ${lrPrCats * 100.0}%2.4f%%\\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesModel\n",
      "Accuracy: 60.9601%\n",
      "Area under PR: 74.0522%\n",
      "Area under ROC: 60.5138%\n"
     ]
    }
   ],
   "source": [
    "//Retrain Naive Bayes only with category feature\n",
    "val dataNB = records.map{ r =>\n",
    "    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
    "    val label = trimmed(r.size - 1).toInt\n",
    "    val categoryIdx = categories(r(3))\n",
    "    val categoryFeatures = Array.ofDim[Double](numCategories)\n",
    "    categoryFeatures(categoryIdx) = 1.0\n",
    "    LabeledPoint(label, Vectors.dense(categoryFeatures))\n",
    "}\n",
    "\n",
    "val nbModelCats = NaiveBayes.train(dataNB)\n",
    "val nbTotalCorrectCats = dataNB.map{ point => \n",
    "    if(nbModelCats.predict(point.features) == point.label) 1 else 0\n",
    "}.sum\n",
    "val nbAccuracyCats = nbTotalCorrectCats / numData\n",
    "val nbPredictionsVsTrueCats = dataNB.map { point =>\n",
    "    (nbModelCats.predict(point.features), point.label)\n",
    "}\n",
    "val nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats)\n",
    "val nbPrCats = nbMetricsCats.areaUnderPR\n",
    "val nbRocCats = nbMetricsCats.areaUnderROC\n",
    "println(f\"${nbModelCats.getClass.getSimpleName}\\nAccuracy: ${nbAccuracyCats * 100}%2.4f%%\\nArea under PR: ${nbPrCats * 100.0}%2.4f%%\\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iterations, AUC = 64.95%\n",
      "5 iterations, AUC = 66.62%\n",
      "10 iterations, AUC = 66.55%\n",
      "50 iterations, AUC = 66.81%\n"
     ]
    }
   ],
   "source": [
    "//Tuning Model Parameters\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.optimization.Updater\n",
    "import org.apache.spark.mllib.optimization.SimpleUpdater\n",
    "import org.apache.spark.mllib.optimization.L1Updater\n",
    "import org.apache.spark.mllib.optimization.SquaredL2Updater\n",
    "import org.apache.spark.mllib.classification.ClassificationModel\n",
    "\n",
    "def trainWithParams(input: RDD[LabeledPoint], regParam: Double, numIterations: Int, updater: Updater, stepSize: Double) = {\n",
    "    var lr = new LogisticRegressionWithSGD\n",
    "    lr.optimizer.setNumIterations(numIterations).setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)\n",
    "    lr.run(input)\n",
    "}\n",
    "\n",
    "def createMetrics(label: String, data: RDD[LabeledPoint], model: ClassificationModel) = {\n",
    "    val scoreAndLabels = data.map{ point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (label, metrics.areaUnderROC)\n",
    "}\n",
    "\n",
    "scaledDataCats.cache\n",
    "\n",
    "val iterResults = Seq(1, 5, 10, 50).map{ param =>\n",
    "    val model = trainWithParams(scaledDataCats, 0.0, param, new SimpleUpdater, 1.0)\n",
    "    createMetrics(s\"$param iterations\", scaledDataCats, model)\n",
    "}\n",
    "iterResults.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 step size, AUC = 64.97%\n",
      "0.01 step size, AUC = 64.96%\n",
      "0.1 step size, AUC = 65.52%\n",
      "1.0 step size, AUC = 66.55%\n",
      "10.0 step size, AUC = 61.92%\n"
     ]
    }
   ],
   "source": [
    "val stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map{ param =>\n",
    "    val model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param)\n",
    "    createMetrics(s\"$param step size\", scaledDataCats, model)\n",
    "}\n",
    "stepResults.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 L2 regularization parameter, AUC = 66.55%\n",
      "0.01 L2 regularization parameter, AUC = 66.55%\n",
      "0.1 L2 regularization parameter, AUC = 66.63%\n",
      "1.0 L2 regularization parameter, AUC = 66.04%\n",
      "10.0 L2 regularization parameter, AUC = 35.33%\n"
     ]
    }
   ],
   "source": [
    "//Regularization\n",
    "val regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map{ param =>\n",
    "    val model = trainWithParams(scaledDataCats, param, numIterations, new SquaredL2Updater, 1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", scaledDataCats, model)\n",
    "}\n",
    "regResults.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Tuning Tree Depth and Impurity\n",
    "import org.apache.spark.mllib.tree.impurity.Impurity\n",
    "import org.apache.spark.mllib.tree.impurity.Entropy\n",
    "import org.apache.spark.mllib.tree.impurity.Gini\n",
    "\n",
    "def trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity: Impurity) = {\n",
    "    DecisionTree.train(input, Algo.Classification, impurity, maxDepth)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tree depth, AUC = 59.33%\n",
      "2 tree depth, AUC = 61.68%\n",
      "3 tree depth, AUC = 62.61%\n",
      "4 tree depth, AUC = 63.63%\n",
      "5 tree depth, AUC = 64.89%\n",
      "10 tree depth, AUC = 78.37%\n",
      "20 tree depth, AUC = 98.87%\n"
     ]
    }
   ],
   "source": [
    "val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map{ param =>\n",
    "    val model = trainDTWithParams(data, param, Gini)\n",
    "    val scoreAndLabels = data.map{ point =>\n",
    "        val score = model.predict(point.features)\n",
    "        (if(score > 0.5) 1.0 else 0.0, point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param tree depth\", metrics.areaUnderROC)\n",
    "}\n",
    "dtResultsEntropy.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 lambda, AUC = 60.5138494154944550000000%\n",
      "0.01 lambda, AUC = 60.5138494154944550000000%\n",
      "0.1 lambda, AUC = 60.5138494154944550000000%\n",
      "1.0 lambda, AUC = 60.5138494154944550000000%\n",
      "10.0 lambda, AUC = 60.5138494154944550000000%\n"
     ]
    }
   ],
   "source": [
    "//Naive Bayes parameter tuning\n",
    "def trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {\n",
    "    val nb = new NaiveBayes\n",
    "    nb.setLambda(lambda)\n",
    "    nb.run(input)\n",
    "}\n",
    "\n",
    "val nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
    "    val model = trainNBWithParams(dataNB, param)\n",
    "    val scoreAndLabels = dataNB.map { point =>\n",
    "        (model.predict(point.features), point.label)\n",
    "    }\n",
    "    val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "    (s\"$param lambda\", metrics.areaUnderROC)\n",
    "}\n",
    "nbResults.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.22f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 L2 regularization parameter, AUC = 66.126842%\n",
      "0.001 L2 regularization parameter, AUC = 66.126842%\n",
      "0.0025 L2 regularization parameter, AUC = 66.126842%\n",
      "0.005 L2 regularization parameter, AUC = 66.126842%\n",
      "0.01 L2 regularization parameter, AUC = 66.093195%\n"
     ]
    }
   ],
   "source": [
    "//Cross-Validation\n",
    "val trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 123)\n",
    "val train = trainTestSplit(0)\n",
    "val test = trainTestSplit(1)\n",
    "\n",
    "val regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map{ param =>\n",
    "    val model = trainWithParams(train, param, numIterations, new SquaredL2Updater, 1.0)\n",
    "    createMetrics(s\"$param L2 regularization parameter\", test, model)\n",
    "}\n",
    "regResultsTest.foreach{ case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
